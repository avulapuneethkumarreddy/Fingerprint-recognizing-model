{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8578472,"sourceType":"datasetVersion","datasetId":5130067}],"dockerImageVersionId":30207,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Setup and Configuration","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, models, transforms\nfrom sklearn.metrics import accuracy_score\nfrom PIL import Image\nimport os\nimport copy\nimport random\nimport numpy as np\n\n# --------------------------\n# Set random seeds for reproducibility\n# --------------------------\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\n# --------------------------\n# Paths and device setup\n# --------------------------\ndata_dir = \"/kaggle/input/finger-prints-1-10/Finger print Dataset (1-10)\"  # âš ï¸ Replace if needed\naugmented_dir = \"augmented_data\"\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"âœ… Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:02:28.886237Z","iopub.execute_input":"2025-10-25T14:02:28.887095Z","iopub.status.idle":"2025-10-25T14:02:31.149768Z","shell.execute_reply.started":"2025-10-25T14:02:28.887062Z","shell.execute_reply":"2025-10-25T14:02:31.148785Z"}},"outputs":[{"name":"stdout","text":"âœ… Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# 2. Data Augmentation (10Ã— Each Image)","metadata":{}},{"cell_type":"code","source":"augment = transforms.Compose([\n    transforms.RandomResizedCrop(300, scale=(0.8, 1.0)),\n    transforms.RandomRotation(30),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n    transforms.RandomAffine(degrees=0, shear=10, translate=(0.1, 0.1)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomPerspective(distortion_scale=0.4, p=0.7),\n    transforms.RandomAdjustSharpness(sharpness_factor=2),\n])\n\nif not os.path.exists(augmented_dir):\n    print(\"ðŸ§¬ Creating augmented dataset...\")\n    os.makedirs(augmented_dir, exist_ok=True)\n    \n    for class_name in os.listdir(data_dir):\n        class_path = os.path.join(data_dir, class_name)\n        if not os.path.isdir(class_path):\n            continue\n        \n        dest_dir = os.path.join(augmented_dir, class_name)\n        os.makedirs(dest_dir, exist_ok=True)\n        \n        for img_name in os.listdir(class_path):\n            img_path = os.path.join(class_path, img_name)\n            img = Image.open(img_path).convert(\"RGB\")\n            \n            # Save original\n            img.save(os.path.join(dest_dir, img_name))\n            \n            # Generate 10 augmented copies\n            for i in range(10):\n                aug_img = augment(img)\n                aug_img.save(os.path.join(dest_dir, f\"{os.path.splitext(img_name)[0]}_aug{i}.jpg\"))\n\n    print(\"âœ… Augmented dataset created successfully!\")\nelse:\n    print(\"ðŸ” Augmented dataset already exists â€” skipping creation.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:02:31.151269Z","iopub.execute_input":"2025-10-25T14:02:31.151730Z","iopub.status.idle":"2025-10-25T14:02:48.115269Z","shell.execute_reply.started":"2025-10-25T14:02:31.151693Z","shell.execute_reply":"2025-10-25T14:02:48.114349Z"}},"outputs":[{"name":"stdout","text":"ðŸ§¬ Creating augmented dataset...\nâœ… Augmented dataset created successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 3. Data Transforms (Train / Validation)","metadata":{}},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    transforms.Grayscale(num_output_channels=3),\n    transforms.RandomResizedCrop(300, scale=(0.8, 1.0)),\n    transforms.RandomRotation(25),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.RandomAffine(degrees=15, shear=8),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomErasing(p=0.3, scale=(0.02, 0.15)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Grayscale(num_output_channels=3),\n    transforms.Resize((300, 300)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:02:48.116368Z","iopub.execute_input":"2025-10-25T14:02:48.116646Z","iopub.status.idle":"2025-10-25T14:02:48.123105Z","shell.execute_reply.started":"2025-10-25T14:02:48.116620Z","shell.execute_reply":"2025-10-25T14:02:48.122246Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# 4. Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"dataset = datasets.ImageFolder(root=augmented_dir, transform=train_transforms)\n\n# Split into train/val (85/15)\ntrain_size = int(0.85 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Apply val transforms\nval_dataset.dataset.transform = val_transforms\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n\nclass_names = dataset.classes\nprint(f\"ðŸ§¾ Classes: {class_names}\")\nprint(f\"ðŸ“Š Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:02:48.124981Z","iopub.execute_input":"2025-10-25T14:02:48.125237Z","iopub.status.idle":"2025-10-25T14:02:48.142036Z","shell.execute_reply.started":"2025-10-25T14:02:48.125214Z","shell.execute_reply":"2025-10-25T14:02:48.140833Z"}},"outputs":[{"name":"stdout","text":"ðŸ§¾ Classes: ['001', '002', '003', '004', '005', '006', '007', '008', '009', '010']\nðŸ“Š Train samples: 561, Validation samples: 99\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# 5. Model Definition (EfficientNet-B0)","metadata":{}},{"cell_type":"code","source":"model = models.efficientnet_b0(pretrained=True)\n\n# Freeze feature extractor\nfor param in model.features.parameters():\n    param.requires_grad = False\n\n# Replace classifier head\nnum_features = model.classifier[1].in_features\nmodel.classifier[1] = nn.Sequential(\n    nn.Dropout(0.6),\n    nn.Linear(num_features, len(class_names))\n)\n\n# Multi-GPU support\nif torch.cuda.device_count() > 1:\n    print(f\"ðŸš€ Using {torch.cuda.device_count()} GPUs!\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:02:48.143350Z","iopub.execute_input":"2025-10-25T14:02:48.143601Z","iopub.status.idle":"2025-10-25T14:02:51.570170Z","shell.execute_reply.started":"2025-10-25T14:02:48.143578Z","shell.execute_reply":"2025-10-25T14:02:51.569345Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9892e70deb9d41ea90568b0ba4262dcc"}},"metadata":{}},{"name":"stdout","text":"ðŸš€ Using 2 GPUs!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# 6. Loss, Optimizer, Scheduler, Early Stopping","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n\nbest_acc = 0.0\nepochs_no_improve = 0\npatience = 6\nbest_model_wts = copy.deepcopy(model.state_dict())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:02:51.571253Z","iopub.execute_input":"2025-10-25T14:02:51.571605Z","iopub.status.idle":"2025-10-25T14:02:51.622406Z","shell.execute_reply.started":"2025-10-25T14:02:51.571572Z","shell.execute_reply":"2025-10-25T14:02:51.621805Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 7. Training Loop","metadata":{}},{"cell_type":"code","source":"epochs = 50\nfor epoch in range(epochs):\n    model.train()\n    running_loss, running_corrects = 0, 0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n\n    train_loss = running_loss / len(train_loader.dataset)\n    train_acc = running_corrects.double() / len(train_loader.dataset)\n\n    # ---- Validation ----\n    model.eval()\n    val_loss, val_corrects = 0, 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            _, preds = torch.max(outputs, 1)\n            val_loss += loss.item() * inputs.size(0)\n            val_corrects += torch.sum(preds == labels.data)\n\n    val_loss /= len(val_loader.dataset)\n    val_acc = val_corrects.double() / len(val_loader.dataset)\n    scheduler.step(val_acc)\n\n    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n          f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n\n    # Save best model\n    if val_acc > best_acc:\n        best_acc = val_acc\n        best_model_wts = copy.deepcopy(model.state_dict())\n        torch.save(model.state_dict(), \"best_fingerprint_model_final.pth\")\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n\n    if epochs_no_improve >= patience:\n        print(\"ðŸ›‘ Early stopping triggered!\")\n        break\n\nprint(f\"âœ… Best Validation Accuracy: {best_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:02:51.623455Z","iopub.execute_input":"2025-10-25T14:02:51.623816Z","iopub.status.idle":"2025-10-25T14:04:43.400548Z","shell.execute_reply.started":"2025-10-25T14:02:51.623779Z","shell.execute_reply":"2025-10-25T14:04:43.399410Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/50] | Train Acc: 0.1301 | Val Acc: 0.1616 | LR: 0.000100\nEpoch [2/50] | Train Acc: 0.1711 | Val Acc: 0.3232 | LR: 0.000100\nEpoch [3/50] | Train Acc: 0.1925 | Val Acc: 0.4646 | LR: 0.000100\nEpoch [4/50] | Train Acc: 0.2406 | Val Acc: 0.5051 | LR: 0.000100\nEpoch [5/50] | Train Acc: 0.2692 | Val Acc: 0.6061 | LR: 0.000100\nEpoch [6/50] | Train Acc: 0.3387 | Val Acc: 0.6263 | LR: 0.000100\nEpoch [7/50] | Train Acc: 0.3886 | Val Acc: 0.6667 | LR: 0.000100\nEpoch [8/50] | Train Acc: 0.3922 | Val Acc: 0.6465 | LR: 0.000100\nEpoch [9/50] | Train Acc: 0.4599 | Val Acc: 0.7273 | LR: 0.000100\nEpoch [10/50] | Train Acc: 0.4617 | Val Acc: 0.7273 | LR: 0.000100\nEpoch [11/50] | Train Acc: 0.4742 | Val Acc: 0.7677 | LR: 0.000100\nEpoch [12/50] | Train Acc: 0.5455 | Val Acc: 0.7879 | LR: 0.000100\nEpoch [13/50] | Train Acc: 0.5740 | Val Acc: 0.7778 | LR: 0.000100\nEpoch [14/50] | Train Acc: 0.5722 | Val Acc: 0.7778 | LR: 0.000100\nEpoch [15/50] | Train Acc: 0.5971 | Val Acc: 0.7576 | LR: 0.000100\nEpoch [16/50] | Train Acc: 0.6096 | Val Acc: 0.8384 | LR: 0.000100\nEpoch [17/50] | Train Acc: 0.6275 | Val Acc: 0.7980 | LR: 0.000100\nEpoch [18/50] | Train Acc: 0.6631 | Val Acc: 0.8081 | LR: 0.000100\nEpoch [19/50] | Train Acc: 0.6435 | Val Acc: 0.7980 | LR: 0.000100\nEpoch 00020: reducing learning rate of group 0 to 5.0000e-05.\nEpoch [20/50] | Train Acc: 0.6453 | Val Acc: 0.8081 | LR: 0.000050\nEpoch [21/50] | Train Acc: 0.6863 | Val Acc: 0.7980 | LR: 0.000050\nEpoch [22/50] | Train Acc: 0.6774 | Val Acc: 0.8283 | LR: 0.000050\nðŸ›‘ Early stopping triggered!\nâœ… Best Validation Accuracy: 0.8384\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# 8. Evaluation and Final Accuracy","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(best_model_wts)\nmodel.eval()\nall_preds, all_labels = [], []\n\nwith torch.no_grad():\n    for inputs, labels in val_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\nfinal_acc = accuracy_score(all_labels, all_preds)\nprint(f\"ðŸ”¥ Final Validation Accuracy: {final_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:04:43.402241Z","iopub.execute_input":"2025-10-25T14:04:43.403188Z","iopub.status.idle":"2025-10-25T14:04:44.183859Z","shell.execute_reply.started":"2025-10-25T14:04:43.403142Z","shell.execute_reply":"2025-10-25T14:04:44.182697Z"}},"outputs":[{"name":"stdout","text":"ðŸ”¥ Final Validation Accuracy: 0.8384\n","output_type":"stream"}],"execution_count":9}]}